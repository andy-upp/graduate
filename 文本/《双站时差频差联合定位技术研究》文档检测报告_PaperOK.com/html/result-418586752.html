<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<title>无标题文档</title>
		<link href="img/style.css" rel="stylesheet" type="text/css" media="all" />
	</head>
	<body id="shard-result">
		<div class="mainbody result">
			<strong class="title">原文句子<a href="#modifyAdvise">（查看句子修改意见）</a></strong>
			<div class="p source">
				（2）相比于Tanh函数和Sigmoid函数而言，ReLU函数只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。
			</div>
			<strong class="title">片段位置图</strong>
			<p>
				<img src="img/localimg_418586752.png"/>
			</p>
			<strong class="title">相似结果</strong>
			<table class="detail">
				<tbody>
					<tr>
						<td valign="top" class="result">
									<div style="width:20px;height:20px;border:1px solid #ccc;line-height:20px;text-align:center;margin:10px 0px;font-size:14px;font-weight:bold;">
										1
									</div>
									<div class="p" style="background:#ccc;">
										<b>原句片段：</b>（2）相比于Tanh函数和Sigmoid函数而言，ReLU函数只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。
									</div>
											<div class="p">
												<b>相似片段 1：</b>tanh <em>函数实际上是一个放大的 sigmoid 函数,数学关系为:$$\tanh \left( x\...相比于 sigmoid/tanh , ReLU 只需要一个阈值就可以得到激活值,而不用去算一大</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《[机器学习]常用激活函数的总结与比较 - V2EX》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															75%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://www.v2ex.com/t/340003" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 2：</b>日常coding 中,我们会很自然的使用一些<em>激活函数,比如:sigmoid、ReLU等等。不过好像...相比于 sigmoid/tanh,ReLU 只需要一个阈值就可以得到激活值,而不用去算一大堆</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络之激活函数(sigmoid、tanh、ReLU) - CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															75%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/suixinsuiyuan33/article/details/69062894?locationNum=4&fps=1" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 3：</b>RELU 取代 sigmoid 和 tanh <em>函数的原因是在求解梯度下降时 RELU 的速度更快, ...相比于 sigmoid/tanh,ReLU 只需要一个阈值就可以得到激活值,而不用去</em>算一 大...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络激活函数_百度文库》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															75%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://wenku.baidu.com/view/c81c2fcaa0c7aa00b52acfc789eb172ded639938.html" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 4：</b>日常coding 中,我们会很自然的使用一些<em>激活函数,比如:sigmoid、ReLU等等。不过好像...相比于 sigmoid/tanh,ReLU 只需要一个阈值就可以得到激活值,而不用去算一大堆</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《激活函数(Activation Function) - _harvey - 博客园》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															75%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://www.cnblogs.com/harvey888/p/7236537.html" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 5：</b>2. sigmoid <em>函数不是关于原点中心对称的 这个特性会导致后面网络层的输入也不是...相比于 sigmoid/tanh,ReLU 只需要一个阈值就可以得到激活值,而不用去算一大堆</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《常用激活函数的总结与比较 - 知乎专栏》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															75%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://zhuanlan.zhihu.com/p/25204824" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 6：</b>日常coding 中,我们会很自然的使用一些<em>激活函数,比如:sigmoid、ReLU等等。不过...相比于 sigmoid/tanh,ReLU 只需要一个阈值就可以得到激活值,而不用去算一大堆</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络之激活函数(Activation Function) - CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															75%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/memray/article/details/51442059" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 7：</b>日常coding 中,我们会很自然的使用一些<em>激活函数,比如:sigmoid、ReLU等等。不过...相比于 sigmoid/tanh,ReLU 只需要一个阈值就可以得到激活值,而不用去算一大堆</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《RELU 激活函数及其他相关的函数 - CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															75%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/u013146742/article/details/51986575" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 8：</b>使用ReLU <em>得到的SGD的收敛速度会比 sigmoid/tanh 快很多(看右图)。相比于 sigmoid/tanh,ReLU 只需要一个阈值就可以得到激活值,而不用去算一大堆复杂的运算</em>。 ReL...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《深度学习——激活函数Sigmoid/Tanh/ReLU - zchang81的..._CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															65%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/zchang81/article/details/70224688" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 9：</b>Tanh. Tanh和Sigmoid是有异曲同工之妙的,它的图形...复杂度高,ReLU <em>只需要一个阈值就可以得到激活值。...激活函数概述 identiti函数 sigmoid函数 relu函数</em> leaky...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《【Stanford CNN课程笔记】5. 神经网络解读1 几种常见的激活函数》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															60%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/elaine_bao/article/details/50810598" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
						</td>
					</tr>
				</tbody>
			</table>
				<a name="modifyAdvise"></a>
				<div class="p"
					style="background: #FFFFE1; border: 1px dashed #FF9797;">
					<p style="font-size: 14px; font-weight: bold;">※ 片段修改建议 ※</p>
					<b>近似词参考：</b><ul type='1'><li>相比：比拟 </li><li>就可以：就能够 就能 </li><li>得到：获得 </li><li>不用：不消 </li><li>复杂：庞大 繁杂 </li></ul><p><b>系统自动生成语句：</b>（2）<u>比拟</u>于Tanh函数和Sigmoid函数而言，ReLU函数只需要一个阈值<u>就能够</u><u>获得</u>激活值，而<u>不消</u>去算一大堆<u>庞大</u>的运算。</p><p class='gray'>注：本片段修改建议为系统自动生成，仅供参考。	</p>
				</div>
		</div>
	</body>
</html>

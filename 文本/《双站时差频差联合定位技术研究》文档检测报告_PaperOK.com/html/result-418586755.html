<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<title>无标题文档</title>
		<link href="img/style.css" rel="stylesheet" type="text/css" media="all" />
	</head>
	<body id="shard-result">
		<div class="mainbody result">
			<strong class="title">原文句子<a href="#modifyAdvise">（查看句子修改意见）</a></strong>
			<div class="p source">
				当然ReLU函数也有它的缺点，就是训练的时候会导致梯度更新很容易“死亡”，比如一个非常大的梯度流过一个ReLU神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都是0。
			</div>
			<strong class="title">片段位置图</strong>
			<p>
				<img src="img/localimg_418586755.png"/>
			</p>
			<strong class="title">相似结果</strong>
			<table class="detail">
				<tbody>
					<tr>
						<td valign="top" class="result">
									<div style="width:20px;height:20px;border:1px solid #ccc;line-height:20px;text-align:center;margin:10px 0px;font-size:14px;font-weight:bold;">
										1
									</div>
									<div class="p" style="background:#ccc;">
										<b>原句片段：</b>梯度流过一个ReLU神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都是0。
									</div>
											<div class="p">
												<b>相似片段 1：</b>举个例子:<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。如果这个情况发生了,那么这个神经元的梯度就永远</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《【机器学习】神经网络-激活函数-面面观(Activation Fun..._CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/cyh_24/article/details/50593400" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 2：</b>举例来说:<em>一个非常大的梯度经过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。如果这种情况发生,那么从此所有流过这个神经元的梯度</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《[机器学习]常用激活函数的总结与比较 - V2EX》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://www.v2ex.com/t/340003" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 3：</b>举个例子:<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。 如果这个情况发生了,那么这个神经元的梯度就永远</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《为什么在生成对抗网络(GAN)中,隐藏层中使用leaky relu比relu要好?》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://www.zhihu.com/question/68514413/answer/268088852" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 4：</b>举个例子:<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。如果这个情况发生了,那么这个神经元的梯度就永远</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《深度学习补充和总结 - AI-ML-DL - 博客园》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://www.cnblogs.com/taojake-ML/p/6382930.html" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 5：</b>举个例子:<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。 如果这个情况发生了,那么这个神经元的梯度就永远</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络之激活函数(sigmoid、tanh、ReLU) - CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/suixinsuiyuan33/article/details/69062894?locationNum=4&fps=1" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 6：</b>举例来说:<em>一个非常大的梯度经过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。如果这种情况发生,那么从此所有流过这个神经元的梯度</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络中各种激活函数比较 - 合唱团abc - 博客园》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/7353331.html" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 7：</b>举个例子:<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神 经元再也不会对任何数据有激活现象了。 如果这个情况发生了,那么这个神经元的梯度</em>就...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络激活函数_百度文库》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://wenku.baidu.com/view/c81c2fcaa0c7aa00b52acfc789eb172ded639938.html" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 8：</b>举个例子:<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。 如果这个情况发生了,那么这个神经元的梯度就永远</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《【神经网络】激活函数面面观 - 知乎专栏》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://zhuanlan.zhihu.com/p/21568660?refer=cyh24" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 9：</b>缺点:例如,<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了,那么这个神经元的梯度就永远</em>都会是 0. 如果learnin...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络简介及CNN - 简书》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://www.jianshu.com/p/8a26eff224d3" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 10：</b>举个例子:<em>一个非常大的梯度流过一个 ReLU 神经元,更新过参数之后,这个神经元再也不会对任何数据有激活现象了。如果这个情况发生了,那么这个神经元的梯度就永远</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《神经网络之激活函数(Activation Function) - CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															95.65%
																<span class="autotype">（高度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://blog.csdn.net/memray/article/details/51442059" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
									<div style="width:20px;height:20px;border:1px solid #ccc;line-height:20px;text-align:center;margin:10px 0px;font-size:14px;font-weight:bold;">
										2
									</div>
									<div class="p" style="background:#ccc;">
										<b>原句片段：</b>当然ReLU函数也有它的缺点，就是训练的时候会导致梯度更新很容易“死亡”，比如一个非常大的
									</div>
											<div class="p">
												<b>相似片段 1：</b>我们会很自然的使用一些激活<em>函数,比如:sigmoid、ReLU...ReLU 的缺点:当然 ReLU 也有缺点,就是训练的时候很...举个例子:一个非常大的梯度流过一个</em> ReLU 神经元...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《【神经网络】激活函数面面观 - 知乎专栏》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															70.59%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="https://zhuanlan.zhihu.com/p/21568660?refer=cyh24" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
											<div class="p">
												<b>相似片段 2：</b>还被作用了<em>一个函数,这个函数就是激活函数 ...Sigmoid 的饱和性虽然会导致梯度消失,但也有其有利...当然ReLU 也有缺点,就是训练的时候很”脆弱”,很容易</em>...
											</div>
											<table class="detail none" border="0">
												<tr><td width="60">篇名</td><td>《深度学习:神经网络中的激活函数 - 皮皮blog - CSDN博客》</td></tr>
												<tr><td>对比库</td><td>
互联网学术资源库</td></tr>																									<tr>
													<td>相似率</td>
													<td>
															64.71%
																<span class="light_autotype">（中度相似）</span>
													</td>
												</tr>
																<tr>
																	<td>来源</td>
																	<td>
																		<span class="cloud">云论文库</span>
																		<a href="http://m.blog.csdn.net/pipisorry/article/details/52102805" target="_blank" class="cloud">查看来源</a>
																	</td>
																</tr>
											</table>
						</td>
					</tr>
				</tbody>
			</table>
				<a name="modifyAdvise"></a>
				<div class="p"
					style="background: #FFFFE1; border: 1px dashed #FF9797;">
					<p style="font-size: 14px; font-weight: bold;">※ 片段修改建议 ※</p>
					<b>近似词参考：</b><ul type='1'><li>当然：固然 </li><li>缺点：错误谬误 弱点 瑕玷 缺陷 </li><li>就是：便是 </li><li>训练：练习 </li><li>时候：时辰 时刻 时间 </li><li>导致：致使 </li><li>容易：轻易 </li><li>死亡：灭亡 殒命 </li><li>比如：好比 譬如 </li><li>之后：以后 </li><li>据有：占有 </li><li>现象：征象 </li><li>那么：那末 </li></ul><p><b>系统自动生成语句：</b><u>固然</u>ReLU函数也有它的<u>错误谬误</u>，<u>便是</u><u>练习</u>的<u>时辰</u>会<u>致使</u>梯度更新很<u>轻易</u>“<u>灭亡</u>”，<u>好比</u>一个非常大的梯度流过一个ReLU神经元，更新过参数<u>以后</u>，这个神经元再也不会对任何数<u>占有</u>激活<u>征象</u>了，<u>那末</u>这个神经元的梯度就永远都是0。</p><p class='gray'>注：本片段修改建议为系统自动生成，仅供参考。	</p>
				</div>
		</div>
	</body>
</html>
